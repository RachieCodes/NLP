{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_ICE_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachiesqueek/NPL/blob/main/NLP_ICE_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZzm4z0Gxs9_",
        "outputId": "b62aa20d-1dc3-4a0f-9512-71eb2f654334"
      },
      "source": [
        "!pip install --force-reinstall nltk[all]\n",
        "!pip install keras.utils \n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk[all]\n",
            "  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "Collecting regex>=2021.8.3\n",
            "  Using cached regex-2021.10.23-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
            "Collecting click\n",
            "  Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "Collecting python-crfsuite\n",
            "  Using cached python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "Collecting twython\n",
            "  Using cached twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting pyparsing\n",
            "  Using cached pyparsing-3.0.0-py3-none-any.whl (95 kB)\n",
            "Collecting gensim<4.0.0\n",
            "  Using cached gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.4.3-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\n",
            "Collecting six>=1.5.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "Collecting importlib-metadata\n",
            "  Using cached importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting python-dateutil>=2.7\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pillow>=6.2.0\n",
            "  Using cached Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting requests-oauthlib>=0.4.0\n",
            "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "Installing collected packages: zipp, urllib3, typing-extensions, idna, charset-normalizer, certifi, six, requests, oauthlib, numpy, importlib-metadata, tqdm, threadpoolctl, smart-open, scipy, requests-oauthlib, regex, python-dateutil, pyparsing, pillow, kiwisolver, joblib, cycler, click, twython, scikit-learn, python-crfsuite, nltk, matplotlib, gensim\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.6.0\n",
            "    Uninstalling zipp-3.6.0:\n",
            "      Successfully uninstalled zipp-3.6.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.7\n",
            "    Uninstalling urllib3-1.26.7:\n",
            "      Successfully uninstalled urllib3-1.26.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.3\n",
            "    Uninstalling idna-3.3:\n",
            "      Successfully uninstalled idna-3.3\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.7\n",
            "    Uninstalling charset-normalizer-2.0.7:\n",
            "      Successfully uninstalled charset-normalizer-2.0.7\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.26.0\n",
            "    Uninstalling requests-2.26.0:\n",
            "      Successfully uninstalled requests-2.26.0\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.1.1\n",
            "    Uninstalling oauthlib-3.1.1:\n",
            "      Successfully uninstalled oauthlib-3.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.3\n",
            "    Uninstalling numpy-1.21.3:\n",
            "      Successfully uninstalled numpy-1.21.3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.1\n",
            "    Uninstalling importlib-metadata-4.8.1:\n",
            "      Successfully uninstalled importlib-metadata-4.8.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.0.0\n",
            "    Uninstalling threadpoolctl-3.0.0:\n",
            "      Successfully uninstalled threadpoolctl-3.0.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 5.2.1\n",
            "    Uninstalling smart-open-5.2.1:\n",
            "      Successfully uninstalled smart-open-5.2.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.1\n",
            "    Uninstalling scipy-1.7.1:\n",
            "      Successfully uninstalled scipy-1.7.1\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.0\n",
            "    Uninstalling requests-oauthlib-1.3.0:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2021.10.23\n",
            "    Uninstalling regex-2021.10.23:\n",
            "      Successfully uninstalled regex-2021.10.23\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.0\n",
            "    Uninstalling pyparsing-3.0.0:\n",
            "      Successfully uninstalled pyparsing-3.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.3.2\n",
            "    Uninstalling kiwisolver-1.3.2:\n",
            "      Successfully uninstalled kiwisolver-1.3.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.10.0\n",
            "    Uninstalling cycler-0.10.0:\n",
            "      Successfully uninstalled cycler-0.10.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.0.3\n",
            "    Uninstalling click-8.0.3:\n",
            "      Successfully uninstalled click-8.0.3\n",
            "  Attempting uninstall: twython\n",
            "    Found existing installation: twython 3.9.1\n",
            "    Uninstalling twython-3.9.1:\n",
            "      Successfully uninstalled twython-3.9.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0\n",
            "    Uninstalling scikit-learn-1.0:\n",
            "      Successfully uninstalled scikit-learn-1.0\n",
            "  Attempting uninstall: python-crfsuite\n",
            "    Found existing installation: python-crfsuite 0.9.7\n",
            "    Uninstalling python-crfsuite-0.9.7:\n",
            "      Successfully uninstalled python-crfsuite-0.9.7\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.6.5\n",
            "    Uninstalling nltk-3.6.5:\n",
            "      Successfully uninstalled nltk-3.6.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.4.3\n",
            "    Uninstalling matplotlib-3.4.3:\n",
            "      Successfully uninstalled matplotlib-3.4.3\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.8.3\n",
            "    Uninstalling gensim-3.8.3:\n",
            "      Successfully uninstalled gensim-3.8.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed certifi-2021.10.8 charset-normalizer-2.0.7 click-8.0.3 cycler-0.10.0 gensim-3.8.3 idna-3.3 importlib-metadata-4.8.1 joblib-1.1.0 kiwisolver-1.3.2 matplotlib-3.4.3 nltk-3.6.5 numpy-1.21.3 oauthlib-3.1.1 pillow-8.4.0 pyparsing-3.0.0 python-crfsuite-0.9.7 python-dateutil-2.8.2 regex-2021.10.23 requests-2.26.0 requests-oauthlib-1.3.0 scikit-learn-1.0 scipy-1.7.1 six-1.16.0 smart-open-5.2.1 threadpoolctl-3.0.0 tqdm-4.62.3 twython-3.9.1 typing-extensions-3.10.0.2 urllib3-1.26.7 zipp-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "charset_normalizer",
                  "cycler",
                  "dateutil",
                  "idna",
                  "joblib",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "nltk",
                  "numpy",
                  "pycrfsuite",
                  "pyparsing",
                  "regex",
                  "requests",
                  "scipy",
                  "six",
                  "sklearn",
                  "tqdm",
                  "typing_extensions",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras.utils\n",
            "  Downloading keras-utils-1.0.13.tar.gz (2.4 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras.utils) (2.6.0)\n",
            "Building wheels for collected packages: keras.utils\n",
            "  Building wheel for keras.utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras.utils: filename=keras_utils-1.0.13-py3-none-any.whl size=2656 sha256=bf07eed33c857ae1ef81b81b024f5fcfcb410491a6dfe3189d2e2241485d0972\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/dd/3b/493952a5240d486a83805d65360dedadbadeae71d25e2c877f\n",
            "Successfully built keras.utils\n",
            "Installing collected packages: keras.utils\n",
            "Successfully installed keras.utils-1.0.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34pFqhz-XANl"
      },
      "source": [
        "# ICE-8\n",
        "# This ICE is pretty straight forward. Please follow the link below and solve all the coding problems that are explained\n",
        "# https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
        "# Once done with the above part, use any dataset of your choice and build your own language model with the techniques covered in the online tutorial.\n",
        "import random\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#(50%)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0x8X-UprZa3",
        "outputId": "6f820bcb-28a3-420b-be09-a11479a740de"
      },
      "source": [
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "      accumulator += model[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today the company said that a date for the whole OECD area , but could not estimate the Fed ' s revised 81 . 5 billion dlrs in cash and credit growth figures released by the U . S . Agriculture Department said .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlluPw8-sxWn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lb-xbBJzEOr"
      },
      "source": [
        "data_text = \"\"\"Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity.\n",
        "\n",
        "But 100 years later, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later...\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: ...the Negro is still languished in the corners of American society and finds himself in exile in his own land. And so we've come here today to dramatize a shameful condition. In a sense we've come to our nation's capital to cash a check. When the architects of our republic wrote the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory note to which every American was to fall heir. This note was a promise that all men - yes, black men as well as white men - would be guaranteed the unalienable rights of life, liberty and the pursuit of happiness.\n",
        "\n",
        "It is obvious today that America has defaulted on this promissory note insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check which has come back marked insufficient funds.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: But we refuse to believe that the bank of justice is bankrupt.\n",
        "\n",
        "SOUNDBITE OF LAUGHTER\n",
        "\n",
        "Dr. KING: We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so we've come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: We have also come to his hallowed spot to remind America of the fierce urgency of now. This is no time to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time...\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: ...to lift our nation from the quick sands of racial injustice to the solid rock of brotherhood. Now is the time to make justice a reality for all of God's children.\n",
        "\n",
        "It would be fatal for the nation to overlook the urgency of the moment. This sweltering summer of the Negro's legitimate discontent will not pass until there is an invigorating autumn of freedom and equality. 1963 is not an end, but a beginning. Those who hope that the Negro needed to blow off steam and will now be content will have a rude awakening if the nation returns to business as usual.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: There will be neither rest nor tranquility in America until the Negro is granted his citizenship rights. The whirlwinds of revolt will continue to shake the foundations of our nation until the bright day of justice emerges.\n",
        "\n",
        "But there is something that I must say to my people who stand on the warm threshold which leads into the palace of justice. In the process of gaining our rightful place, we must not be guilty of wrongful deeds. Let us not seek to satisfy our thirst for freedom by drinking from the cup of bitterness and hatred.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: We must forever conduct our struggle on the high plane of dignity and discipline. We must not allow our creative protest to degenerate into physical violence. Again and again, we must rise to the majestic heights of meeting physical force with soul force. The marvelous new militancy which has engulfed the Negro community must not lead us to a distrust of all white people, for many of our white brothers, as evidenced by their presence here today, have come to realize that their destiny is tied up with our destiny.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: And they have come to realize that their freedom is inextricably bound to our freedom. We cannot walk alone. And as we walk, we must make the pledge that we shall always march ahead. We cannot turn back.\n",
        "\n",
        "There are those who are asking the devotees of civil rights, when will you be satisfied? We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of police brutality. We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in the motels of the highways and the hotels of the cities.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: We cannot be satisfied as long as the Negro's basic mobility is from a smaller ghetto to a larger one. We can never be satisfied as long as our children are stripped of their selfhood and robbed of their dignity by signs stating: for whites only.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: We cannot be satisfied as long as a Negro in Mississippi cannot vote and a Negro in New York believes he has nothing for which to vote.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: No, no, we are not satisfied, and we will not be satisfied until justice rolls down like waters, and righteousness like a mighty stream.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: I am not unmindful that some of you have come here out of great trials and tribulations. Some of you have come fresh from narrow jail cells. Some of you have come from areas where your quest for freedom left you battered by the storms of persecution and staggered by the winds of police brutality. You have been the veterans of creative suffering. Continue to work with the faith that unearned suffering is redemptive. Go back to Mississippi, go back to Alabama, go back to South Carolina, go back to Georgia, go back to Louisiana, go back to the slums and ghettos of our Northern cities, knowing that somehow this situation can and will be changed.\n",
        "\n",
        "Let us not wallow in the valley of despair, I say to you today, my friends.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: So even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream deeply rooted in the American dream. I have a dream that one day this nation will rise up and live out the true meaning of its creed: We hold these truths to be self-evident, that all men are created equal.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.\n",
        "\n",
        "I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression will be transformed into an oasis of freedom and justice.\n",
        "\n",
        "I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: I have a dream that one day down in Alabama with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification, one day right down in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers. I have a dream today.\n",
        "\n",
        "SOUNDBITE OF CHEERS AND APPLAUSE\n",
        "\n",
        "Dr. KING: I have a dream that one day every valley shall be exalted, every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed, and all flesh shall see it together.\n",
        "\n",
        "This is our hope. This is the faith that I go back to the South with. With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\n",
        "\n",
        "SOUNDBITE OF APPLAUSE\n",
        "\n",
        "Dr. KING: This will be the day when all of God's children will be able to sing with new meaning: My country, 'tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrims' pride, from every mountainside, let freedom ring.\n",
        "\n",
        "And if America is to be a great nation, this must become true. And so let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that, let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\n",
        "\n",
        "And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God's children, black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. Free at last. Thank God almighty, we are free at last.\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gep8Nwdb09YJ"
      },
      "source": [
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:                  \n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYI5LC4g1AYN",
        "outputId": "c3fff503-7a0a-4348-caca-196d407ba4b2"
      },
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences   \n",
        "sequences = create_seq(data_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 7960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzjHzz4j1w1A"
      },
      "source": [
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5XfOb0y10xh",
        "outputId": "c9b9dda3-7748-4a06-d805-581538d887ca"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (7164, 30) Val shape: (796, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_K2w9Q61610",
        "outputId": "80342b8b-405f-4d51-8e52-e6b05b3488ce"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 30, 50)            1350      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 150)               90900     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 27)                4077      \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "224/224 - 21s - loss: 2.7781 - acc: 0.2030 - val_loss: 2.4562 - val_acc: 0.2764\n",
            "Epoch 2/100\n",
            "224/224 - 18s - loss: 2.3230 - acc: 0.3114 - val_loss: 2.2245 - val_acc: 0.3166\n",
            "Epoch 3/100\n",
            "224/224 - 18s - loss: 2.1815 - acc: 0.3444 - val_loss: 2.1202 - val_acc: 0.3480\n",
            "Epoch 4/100\n",
            "224/224 - 18s - loss: 2.0680 - acc: 0.3783 - val_loss: 2.0300 - val_acc: 0.3756\n",
            "Epoch 5/100\n",
            "224/224 - 18s - loss: 1.9572 - acc: 0.4256 - val_loss: 1.9586 - val_acc: 0.4108\n",
            "Epoch 6/100\n",
            "224/224 - 18s - loss: 1.8471 - acc: 0.4549 - val_loss: 1.8995 - val_acc: 0.4372\n",
            "Epoch 7/100\n",
            "224/224 - 18s - loss: 1.7440 - acc: 0.4870 - val_loss: 1.8266 - val_acc: 0.4585\n",
            "Epoch 8/100\n",
            "224/224 - 18s - loss: 1.6523 - acc: 0.5094 - val_loss: 1.7979 - val_acc: 0.4661\n",
            "Epoch 9/100\n",
            "224/224 - 18s - loss: 1.5608 - acc: 0.5363 - val_loss: 1.7882 - val_acc: 0.4824\n",
            "Epoch 10/100\n",
            "224/224 - 18s - loss: 1.4857 - acc: 0.5551 - val_loss: 1.7400 - val_acc: 0.4975\n",
            "Epoch 11/100\n",
            "224/224 - 18s - loss: 1.4046 - acc: 0.5817 - val_loss: 1.7257 - val_acc: 0.5163\n",
            "Epoch 12/100\n",
            "224/224 - 18s - loss: 1.3375 - acc: 0.6018 - val_loss: 1.7194 - val_acc: 0.5176\n",
            "Epoch 13/100\n",
            "224/224 - 18s - loss: 1.2734 - acc: 0.6122 - val_loss: 1.7382 - val_acc: 0.5151\n",
            "Epoch 14/100\n",
            "224/224 - 18s - loss: 1.2056 - acc: 0.6329 - val_loss: 1.7328 - val_acc: 0.5201\n",
            "Epoch 15/100\n",
            "224/224 - 18s - loss: 1.1499 - acc: 0.6499 - val_loss: 1.7415 - val_acc: 0.5264\n",
            "Epoch 16/100\n",
            "224/224 - 18s - loss: 1.0977 - acc: 0.6710 - val_loss: 1.7290 - val_acc: 0.5339\n",
            "Epoch 17/100\n",
            "224/224 - 18s - loss: 1.0392 - acc: 0.6820 - val_loss: 1.7327 - val_acc: 0.5264\n",
            "Epoch 18/100\n",
            "224/224 - 18s - loss: 1.0016 - acc: 0.6922 - val_loss: 1.7570 - val_acc: 0.5327\n",
            "Epoch 19/100\n",
            "224/224 - 18s - loss: 0.9508 - acc: 0.7066 - val_loss: 1.7743 - val_acc: 0.5289\n",
            "Epoch 20/100\n",
            "224/224 - 18s - loss: 0.9065 - acc: 0.7272 - val_loss: 1.7708 - val_acc: 0.5389\n",
            "Epoch 21/100\n",
            "224/224 - 18s - loss: 0.8834 - acc: 0.7270 - val_loss: 1.7813 - val_acc: 0.5477\n",
            "Epoch 22/100\n",
            "224/224 - 18s - loss: 0.8372 - acc: 0.7448 - val_loss: 1.8013 - val_acc: 0.5465\n",
            "Epoch 23/100\n",
            "224/224 - 18s - loss: 0.8166 - acc: 0.7462 - val_loss: 1.8429 - val_acc: 0.5327\n",
            "Epoch 24/100\n",
            "224/224 - 18s - loss: 0.7774 - acc: 0.7634 - val_loss: 1.8478 - val_acc: 0.5440\n",
            "Epoch 25/100\n",
            "224/224 - 18s - loss: 0.7533 - acc: 0.7684 - val_loss: 1.8756 - val_acc: 0.5553\n",
            "Epoch 26/100\n",
            "224/224 - 18s - loss: 0.7139 - acc: 0.7781 - val_loss: 1.8958 - val_acc: 0.5490\n",
            "Epoch 27/100\n",
            "224/224 - 18s - loss: 0.6957 - acc: 0.7821 - val_loss: 1.9078 - val_acc: 0.5389\n",
            "Epoch 28/100\n",
            "224/224 - 18s - loss: 0.6739 - acc: 0.7940 - val_loss: 1.9500 - val_acc: 0.5327\n",
            "Epoch 29/100\n",
            "224/224 - 18s - loss: 0.6443 - acc: 0.7990 - val_loss: 1.9450 - val_acc: 0.5377\n",
            "Epoch 30/100\n",
            "224/224 - 18s - loss: 0.6330 - acc: 0.8003 - val_loss: 1.9650 - val_acc: 0.5465\n",
            "Epoch 31/100\n",
            "224/224 - 18s - loss: 0.6149 - acc: 0.8085 - val_loss: 1.9834 - val_acc: 0.5503\n",
            "Epoch 32/100\n",
            "224/224 - 18s - loss: 0.5879 - acc: 0.8162 - val_loss: 2.0226 - val_acc: 0.5465\n",
            "Epoch 33/100\n",
            "224/224 - 18s - loss: 0.5773 - acc: 0.8226 - val_loss: 2.0102 - val_acc: 0.5477\n",
            "Epoch 34/100\n",
            "224/224 - 18s - loss: 0.5643 - acc: 0.8224 - val_loss: 2.0319 - val_acc: 0.5503\n",
            "Epoch 35/100\n",
            "224/224 - 18s - loss: 0.5484 - acc: 0.8266 - val_loss: 2.0630 - val_acc: 0.5440\n",
            "Epoch 36/100\n",
            "224/224 - 18s - loss: 0.5218 - acc: 0.8395 - val_loss: 2.0801 - val_acc: 0.5490\n",
            "Epoch 37/100\n",
            "224/224 - 18s - loss: 0.5174 - acc: 0.8365 - val_loss: 2.1072 - val_acc: 0.5276\n",
            "Epoch 38/100\n",
            "224/224 - 18s - loss: 0.5064 - acc: 0.8438 - val_loss: 2.1438 - val_acc: 0.5352\n",
            "Epoch 39/100\n",
            "224/224 - 18s - loss: 0.4925 - acc: 0.8395 - val_loss: 2.1428 - val_acc: 0.5528\n",
            "Epoch 40/100\n",
            "224/224 - 18s - loss: 0.4891 - acc: 0.8441 - val_loss: 2.1367 - val_acc: 0.5352\n",
            "Epoch 41/100\n",
            "224/224 - 18s - loss: 0.4737 - acc: 0.8539 - val_loss: 2.1638 - val_acc: 0.5452\n",
            "Epoch 42/100\n",
            "224/224 - 18s - loss: 0.4620 - acc: 0.8571 - val_loss: 2.1509 - val_acc: 0.5440\n",
            "Epoch 43/100\n",
            "224/224 - 18s - loss: 0.4504 - acc: 0.8569 - val_loss: 2.2080 - val_acc: 0.5440\n",
            "Epoch 44/100\n",
            "224/224 - 18s - loss: 0.4397 - acc: 0.8629 - val_loss: 2.2306 - val_acc: 0.5402\n",
            "Epoch 45/100\n",
            "224/224 - 18s - loss: 0.4385 - acc: 0.8610 - val_loss: 2.2571 - val_acc: 0.5452\n",
            "Epoch 46/100\n",
            "224/224 - 18s - loss: 0.4390 - acc: 0.8590 - val_loss: 2.2496 - val_acc: 0.5427\n",
            "Epoch 47/100\n",
            "224/224 - 18s - loss: 0.4229 - acc: 0.8656 - val_loss: 2.2613 - val_acc: 0.5415\n",
            "Epoch 48/100\n",
            "224/224 - 18s - loss: 0.4188 - acc: 0.8688 - val_loss: 2.2598 - val_acc: 0.5503\n",
            "Epoch 49/100\n",
            "224/224 - 18s - loss: 0.4110 - acc: 0.8689 - val_loss: 2.3081 - val_acc: 0.5314\n",
            "Epoch 50/100\n",
            "224/224 - 18s - loss: 0.3975 - acc: 0.8712 - val_loss: 2.3596 - val_acc: 0.5427\n",
            "Epoch 51/100\n",
            "224/224 - 18s - loss: 0.3971 - acc: 0.8735 - val_loss: 2.3400 - val_acc: 0.5503\n",
            "Epoch 52/100\n",
            "224/224 - 18s - loss: 0.3914 - acc: 0.8774 - val_loss: 2.3491 - val_acc: 0.5352\n",
            "Epoch 53/100\n",
            "224/224 - 18s - loss: 0.3722 - acc: 0.8811 - val_loss: 2.3785 - val_acc: 0.5327\n",
            "Epoch 54/100\n",
            "224/224 - 18s - loss: 0.3750 - acc: 0.8837 - val_loss: 2.4098 - val_acc: 0.5327\n",
            "Epoch 55/100\n",
            "224/224 - 18s - loss: 0.3766 - acc: 0.8820 - val_loss: 2.4150 - val_acc: 0.5415\n",
            "Epoch 56/100\n",
            "224/224 - 18s - loss: 0.3548 - acc: 0.8854 - val_loss: 2.4440 - val_acc: 0.5352\n",
            "Epoch 57/100\n",
            "224/224 - 18s - loss: 0.3624 - acc: 0.8848 - val_loss: 2.4528 - val_acc: 0.5276\n",
            "Epoch 58/100\n",
            "224/224 - 18s - loss: 0.3642 - acc: 0.8872 - val_loss: 2.4973 - val_acc: 0.5289\n",
            "Epoch 59/100\n",
            "224/224 - 18s - loss: 0.3597 - acc: 0.8853 - val_loss: 2.4993 - val_acc: 0.5364\n",
            "Epoch 60/100\n",
            "224/224 - 18s - loss: 0.3509 - acc: 0.8904 - val_loss: 2.5216 - val_acc: 0.5352\n",
            "Epoch 61/100\n",
            "224/224 - 18s - loss: 0.3520 - acc: 0.8861 - val_loss: 2.5304 - val_acc: 0.5427\n",
            "Epoch 62/100\n",
            "224/224 - 18s - loss: 0.3490 - acc: 0.8878 - val_loss: 2.5167 - val_acc: 0.5389\n",
            "Epoch 63/100\n",
            "224/224 - 18s - loss: 0.3474 - acc: 0.8901 - val_loss: 2.5234 - val_acc: 0.5465\n",
            "Epoch 64/100\n",
            "224/224 - 18s - loss: 0.3406 - acc: 0.8894 - val_loss: 2.5305 - val_acc: 0.5503\n",
            "Epoch 65/100\n",
            "224/224 - 18s - loss: 0.3328 - acc: 0.8929 - val_loss: 2.5583 - val_acc: 0.5503\n",
            "Epoch 66/100\n",
            "224/224 - 18s - loss: 0.3215 - acc: 0.8970 - val_loss: 2.5762 - val_acc: 0.5377\n",
            "Epoch 67/100\n",
            "224/224 - 18s - loss: 0.3291 - acc: 0.8956 - val_loss: 2.5617 - val_acc: 0.5302\n",
            "Epoch 68/100\n",
            "224/224 - 18s - loss: 0.3265 - acc: 0.8945 - val_loss: 2.5916 - val_acc: 0.5415\n",
            "Epoch 69/100\n",
            "224/224 - 18s - loss: 0.3108 - acc: 0.8987 - val_loss: 2.5943 - val_acc: 0.5427\n",
            "Epoch 70/100\n",
            "224/224 - 18s - loss: 0.3252 - acc: 0.8956 - val_loss: 2.6263 - val_acc: 0.5364\n",
            "Epoch 71/100\n",
            "224/224 - 18s - loss: 0.3246 - acc: 0.8932 - val_loss: 2.6165 - val_acc: 0.5452\n",
            "Epoch 72/100\n",
            "224/224 - 18s - loss: 0.3246 - acc: 0.8934 - val_loss: 2.6259 - val_acc: 0.5477\n",
            "Epoch 73/100\n",
            "224/224 - 18s - loss: 0.3138 - acc: 0.8992 - val_loss: 2.6263 - val_acc: 0.5440\n",
            "Epoch 74/100\n",
            "224/224 - 18s - loss: 0.3159 - acc: 0.8971 - val_loss: 2.6398 - val_acc: 0.5452\n",
            "Epoch 75/100\n",
            "224/224 - 18s - loss: 0.3066 - acc: 0.8996 - val_loss: 2.6785 - val_acc: 0.5377\n",
            "Epoch 76/100\n",
            "224/224 - 18s - loss: 0.2970 - acc: 0.9019 - val_loss: 2.7256 - val_acc: 0.5289\n",
            "Epoch 77/100\n",
            "224/224 - 18s - loss: 0.3087 - acc: 0.8989 - val_loss: 2.7207 - val_acc: 0.5339\n",
            "Epoch 78/100\n",
            "224/224 - 18s - loss: 0.3055 - acc: 0.8982 - val_loss: 2.7043 - val_acc: 0.5377\n",
            "Epoch 79/100\n",
            "224/224 - 18s - loss: 0.2937 - acc: 0.9094 - val_loss: 2.7400 - val_acc: 0.5440\n",
            "Epoch 80/100\n",
            "224/224 - 18s - loss: 0.2956 - acc: 0.9017 - val_loss: 2.6996 - val_acc: 0.5377\n",
            "Epoch 81/100\n",
            "224/224 - 18s - loss: 0.2973 - acc: 0.9021 - val_loss: 2.7181 - val_acc: 0.5427\n",
            "Epoch 82/100\n",
            "224/224 - 18s - loss: 0.2878 - acc: 0.9079 - val_loss: 2.7320 - val_acc: 0.5490\n",
            "Epoch 83/100\n",
            "224/224 - 18s - loss: 0.2856 - acc: 0.9095 - val_loss: 2.7304 - val_acc: 0.5389\n",
            "Epoch 84/100\n",
            "224/224 - 18s - loss: 0.2837 - acc: 0.9114 - val_loss: 2.7843 - val_acc: 0.5465\n",
            "Epoch 85/100\n",
            "224/224 - 18s - loss: 0.2811 - acc: 0.9097 - val_loss: 2.7894 - val_acc: 0.5415\n",
            "Epoch 86/100\n",
            "224/224 - 18s - loss: 0.2836 - acc: 0.9077 - val_loss: 2.7596 - val_acc: 0.5452\n",
            "Epoch 87/100\n",
            "224/224 - 18s - loss: 0.2711 - acc: 0.9087 - val_loss: 2.7523 - val_acc: 0.5528\n",
            "Epoch 88/100\n",
            "224/224 - 18s - loss: 0.2942 - acc: 0.9035 - val_loss: 2.8181 - val_acc: 0.5440\n",
            "Epoch 89/100\n",
            "224/224 - 18s - loss: 0.2784 - acc: 0.9118 - val_loss: 2.8128 - val_acc: 0.5402\n",
            "Epoch 90/100\n",
            "224/224 - 18s - loss: 0.2778 - acc: 0.9087 - val_loss: 2.8025 - val_acc: 0.5477\n",
            "Epoch 91/100\n",
            "224/224 - 18s - loss: 0.2776 - acc: 0.9114 - val_loss: 2.8492 - val_acc: 0.5364\n",
            "Epoch 92/100\n",
            "224/224 - 18s - loss: 0.2720 - acc: 0.9101 - val_loss: 2.8280 - val_acc: 0.5389\n",
            "Epoch 93/100\n",
            "224/224 - 18s - loss: 0.2771 - acc: 0.9079 - val_loss: 2.8486 - val_acc: 0.5402\n",
            "Epoch 94/100\n",
            "224/224 - 18s - loss: 0.2738 - acc: 0.9119 - val_loss: 2.7950 - val_acc: 0.5540\n",
            "Epoch 95/100\n",
            "224/224 - 18s - loss: 0.2763 - acc: 0.9104 - val_loss: 2.8538 - val_acc: 0.5389\n",
            "Epoch 96/100\n",
            "224/224 - 18s - loss: 0.2708 - acc: 0.9084 - val_loss: 2.8541 - val_acc: 0.5389\n",
            "Epoch 97/100\n",
            "224/224 - 18s - loss: 0.2606 - acc: 0.9162 - val_loss: 2.8240 - val_acc: 0.5565\n",
            "Epoch 98/100\n",
            "224/224 - 18s - loss: 0.2689 - acc: 0.9109 - val_loss: 2.8625 - val_acc: 0.5440\n",
            "Epoch 99/100\n",
            "224/224 - 18s - loss: 0.2656 - acc: 0.9147 - val_loss: 2.8516 - val_acc: 0.5440\n",
            "Epoch 100/100\n",
            "224/224 - 18s - loss: 0.2669 - acc: 0.9130 - val_loss: 2.8911 - val_acc: 0.5389\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feabe497a90>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFS91e2H94qz"
      },
      "source": [
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict character\n",
        "\t\tyhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0PoN7u-ED3",
        "outputId": "5f5f97a7-80fa-4011-d49e-473d1581948d"
      },
      "source": [
        "inp = 'I have a dream'\n",
        "print(len(inp))\n",
        "print(generate_seq(model,mapping,30,inp.lower(),15))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "i have a dream deeply rooted \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQisc7pyZiIp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a29f6fd5-680d-42a0-e8fa-ea89a68cf919"
      },
      "source": [
        "# Use any dataset and model to calculate Language Model's text level preplexity\n",
        "# You may take help from the following link to have an idea about preplexity.\n",
        "# Use any dataset and model to calculate Language Model's text level preplexity\n",
        "# https://stackoverflow.com/questions/16509685/ngram-model-and-perplexity-in-nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "brown = nltk.corpus.brown\n",
        "corpus = [word.lower() for word in data_text]\n",
        "\n",
        "# Train on 95% f the corpus and test on the rest\n",
        "spl = 95*len(corpus)/100\n",
        "train = corpus[:spl]\n",
        "test = corpus[spl:]\n",
        "\n",
        "# Remove rare words from the corpus\n",
        "fdist = nltk.FreqDist(w for w in train)\n",
        "vocabulary = set(map(lambda x: x[0], filter(lambda x: x[1] >= 5, fdist.iteritems())))\n",
        "\n",
        "train = map(lambda x: x if x in vocabulary else \"*unknown*\", train)\n",
        "test = map(lambda x: x if x in vocabulary else \"*unknown*\", test)\n",
        "\n",
        "\n",
        "from nltk.model import NgramModel\n",
        "from nltk.probability import LidstoneProbDist\n",
        "\n",
        "estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2) \n",
        "lm = NgramModel(5, train, estimator=estimator)\n",
        "\n",
        "print (\"len(corpus) = %s, len(vocabulary) = %s, len(train) = %s, len(test) = %s\" % ( len(corpus), len(vocabulary), len(train), len(test) ))\n",
        "print (\"perplexity(test) =\", lm.perplexity(test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-75e2fb365e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train on 95% f the corpus and test on the rest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mspl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mspl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
          ]
        }
      ]
    }
  ]
}