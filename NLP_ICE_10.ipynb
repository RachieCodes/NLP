{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_ICE_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachiesqueek/NPL/blob/main/NLP_ICE_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBgH0Dc08SeX"
      },
      "source": [
        "#ICE-10: Spelling Correction in Natural Language Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKEF5g3P-r9i"
      },
      "source": [
        "Please follow the tutorials below and complete the tasks that are available on the webpages provided. The tutorials will have code and might not have dataset file. You can create dataset files as per the tutorial requirements. The aim of the ICE is to make the tutorials in executed format. You can also use the github repositories of the authors if they are avilable. It is recommended to run the tutorials and then test it with your own datasets (Custom made). You can use any source on the internet to complete the tasks. For task 4 your have to provide mini-examples to differentiate between non-word and real world spelling corrections. After the code for task 4 please provide a brief explanation for what is the difference and what is your analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAYfpbp-9lMV"
      },
      "source": [
        "# Task 1: (20%)\n",
        "### Use the follwing tutorial to implement spelling checking using Textblob\n",
        "\n",
        "https://stackabuse.com/spelling-correction-in-python-with-textblob/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTX_izw-PG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca43920-719c-47b7-a8e7-5c1fb979c268"
      },
      "source": [
        " pip install textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_YYKrXd-PK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5217ee6-fab2-4225-9420-f245405ee80d"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "with open(\"text.txt\", \"r\") as f:        # Opening the test file with the intention to read\n",
        "    text = f.read()                     # Reading the file\n",
        "    textBlb = TextBlob(text)            # Making our first textblob\n",
        "    textCorrected = textBlb.correct()   # Correcting the text\n",
        "    print(textCorrected)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fellow friend. Now are you doing? I hope you are well!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnM_RPoU-PPS"
      },
      "source": [
        "def compare(text1, text2):  \n",
        "    l1 = text1.split()\n",
        "    l2 = text2.split()\n",
        "    good = 0\n",
        "    bad = 0\n",
        "    for i in range(0, len(l1)):\n",
        "        if l1[i] != l2[i]:\n",
        "            bad += 1\n",
        "        else:\n",
        "            good += 1\n",
        "    return (good, bad)\n",
        "\n",
        "# Helper function to calculate the percentage of misspelled words\n",
        "def percentageOfBad(x):\n",
        "    return (x[1] / (x[0] + x[1])) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UzBEkms-PTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4aef92-bbbc-407d-c56a-328604191fc9"
      },
      "source": [
        "with open(\"text.txt\", \"r\") as f1: # test.txt contains the same typo-filled text from the last example \n",
        "    t1 = f1.read()\n",
        "\n",
        "with open(\"original.txt\", \"r\") as f2: # original.txt contains the text from the actual book \n",
        "    t2 = f2.read()\n",
        "\n",
        "t3 = TextBlob(t1).correct()\n",
        "\n",
        "mistakesCompOriginal = compare(t1, t2)\n",
        "originalCompCorrected = compare(t2, t3)\n",
        "mistakesCompCorrected = compare(t1, t3)\n",
        "\n",
        "print(\"Mistakes compared to original \", mistakesCompOriginal)\n",
        "print(\"Original compared to corrected \", originalCompCorrected)\n",
        "print(\"Mistakes compared to corrected \", mistakesCompCorrected, \"\\n\")\n",
        "\n",
        "print(\"Percentage of mistakes in the test: \", percentageOfBad(mistakesCompOriginal), \"%\")\n",
        "print(\"Percentage of mistakes in the corrected: \", percentageOfBad(originalCompCorrected), \"%\")\n",
        "print(\"Percentage of fixed mistakes: \", percentageOfBad(mistakesCompCorrected), \"%\", \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistakes compared to original  (5, 6)\n",
            "Original compared to corrected  (8, 3)\n",
            "Mistakes compared to corrected  (5, 6) \n",
            "\n",
            "Percentage of mistakes in the test:  54.54545454545454 %\n",
            "Percentage of mistakes in the corrected:  27.27272727272727 %\n",
            "Percentage of fixed mistakes:  54.54545454545454 % \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MXH3VV28eOj"
      },
      "source": [
        "# Task 2: (20%)\n",
        "### Train your model on custom dataset\n",
        "### Instructions are provided in the above *tutorial*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVQ7Td-c-P-W"
      },
      "source": [
        "from textblob.en import Spelling        \n",
        "import re\n",
        "\n",
        "textToLower = \"\"\n",
        "\n",
        "with open(\"cats.txt\",\"r\") as f1:           # Open our source file\n",
        "\ttext = f1.read()                                  # Read the file                 \n",
        "\ttextToLower = text.lower()                        # Lower all the capital letters\n",
        "\n",
        "words = re.findall(\"[a-z]+\", textToLower)             # Find all the words and place them into a list    \n",
        "oneString = \" \".join(words)                           # Join them into one string\n",
        "\n",
        "pathToFile = \"test.txt\"                              # The path we want to store our stats file at\n",
        "spelling = Spelling(path = pathToFile)                # Connect the path to the Spelling object\n",
        "spelling.train(oneString, pathToFile)                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48P-U2uQ-QHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e873ee-1eca-4701-8575-8e1ffe45c3cd"
      },
      "source": [
        "from textblob.en import Spelling        \n",
        "from textblob import TextBlob\n",
        "\n",
        "pathToFile = \"test.txt\" \n",
        "spelling = Spelling(path = pathToFile)\n",
        "text = \" \"\n",
        "\n",
        "with open(\"cats.txt\", \"r\") as f: \n",
        "\ttext = f.read()\n",
        "\n",
        "words = text.split()\n",
        "corrected = \" \"\n",
        "for i in words :\n",
        "    corrected = corrected +\" \"+ spelling.suggest(i)[0][0] # Spell checking word by word\n",
        "\n",
        "print(corrected)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Havign a cta can maen different tihngs to different people Some watn a cat to cuddle and sit on tehir lpas others aer hapyp to live with a very independent cat whihc spneds mots of its tiem otusied and deosn t want too much hmuan interaciton Waht is imoprtnat is that you try to fnid a cat thta will intearct with you if yuo wnat it to All ctas are not the saem and hwo ecah idniviudal cta behaevs with you can dpeend on ist inherent personality and earyl expreiences or lcka of experiecnes whihc can maek it fearful or confidnet with people and lfie in general The environment in which yuo kepe a cat is also extremely significnat for example if it lives with many othre cats which do not gte on thne it wlil be stressde and wlil react differentyl than if it was on its own\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmpmKks9UG38"
      },
      "source": [
        "#Some words are not changed in this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orglcNDl8x6d"
      },
      "source": [
        "# Task 3: (20%)\n",
        "### Implement Petr Norvig algorithm for spelling corrections. The turtorial is provided in the link below\n",
        "\n",
        "https://medium.com/mlearning-ai/build-spell-checking-models-for-any-language-in-python-aa4489df0a5f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11wtQcRJ-Q9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e560215-f7e3-40be-b714-b495fe0f474a"
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 6.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoU9pNZV-RAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd87ba79-4172-4714-c9f8-60b6db607ebd"
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happenning\n",
            "{'happenning', 'hapening'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPZP8pdL-RET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77997e47-40db-4b0f-d945-24d5979a2770"
      },
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "misspell = spell.unknown([\"Havign\", \"a\", \"cta\", \"can\", \"maen\", \"different\", \"tihngs\", \"to\", \"different\", \"people.\", \"Some\", \"watn\", \"a\", \"cat\", \"to\", \"cuddle\", \"and\", \"sit\", \"on\", \"tehir\" \"lpas\"])\n",
        "\n",
        "for word in misspell:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tehirlpas\n",
            "{'tehirlpas'}\n",
            "people\n",
            "{'peopled', 'peoples', 'people', 'peoplel'}\n",
            "man\n",
            "{'amen', 'maken', 'mae', 'mann', 'maven', 'mean', 'maes', 'maan', 'men', \"m'en\", 'mwen', 'marn', 'mien', 'main', 'man', 'meen', 'mane'}\n",
            "things\n",
            "{'tings', 'things'}\n",
            "cat\n",
            "{'cla', 'cpa', 'cia', 'sta', 'cya', 'ca', 'coa', 'cha', 'ata', 'ita', 'ctu', 'eta', 'ota', 'octa', 'pta', 'cea', 'caa', 'cna', 'cua', 'uta', 'cto', 'nta', \"c'a\", 'cat', 'mta', 'tta', 'ta', 'cra'}\n",
            "want\n",
            "{'warn', 'wasn', 'wan', 'watt', 'wat', 'waan', 'wann', 'want', 'wain', 'wats', 'watc', 'wath', 'wate'}\n",
            "having\n",
            "{'having', 'havin'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SMcdxJ7-RHc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYpAfTKD9Kg1"
      },
      "source": [
        "# Task 4: (40%)\n",
        "### Implement spelling correction using Noisy Channel for non-word and real word. You can follow any code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R84pssh4-RxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c8d0b9-810e-46c1-a891-3b86fc39967e"
      },
      "source": [
        "!pip install pyenchant && sudo apt-get install python-enchant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-enchant is already the newest version (2.0.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC8lpGZt-R0V"
      },
      "source": [
        "import enchant\n",
        "import numpy as np\n",
        "import csv\n",
        "import math, collections\n",
        "import pandas as pd\n",
        "import re\n",
        "import itertools\n",
        "from nltk import tokenize\n",
        "import nltk.data\n",
        "\n",
        "\n",
        "class Sentence_Corrector :\n",
        "    def __init__(self, training_file) :\n",
        "        self.laplaceUnigramCounts = collections.defaultdict(lambda: 0)\n",
        "        self.laplaceBigramCounts = collections.defaultdict(lambda: 0)\n",
        "        self.total = 0\n",
        "        self.sentences = []\n",
        "        self.importantKeywords = set()\n",
        "        self.d = enchant.Dict(\"en_US\")\n",
        "        self.tokenize_file(training_file)\n",
        "        self.train()\n",
        "\n",
        "    def tokenize_file(self, file) :\n",
        "        # \"\"\"\n",
        "        #   Read the file, tokenize and build a list of sentences\n",
        "        # \"\"\"\n",
        "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "        f = open(file)\n",
        "        content = f.read()\n",
        "        for sentence in tokenizer.tokenize(content):\n",
        "            sentence_clean = [i.lower() for i in re.split('[^a-zA-Z]+', sentence) if i]\n",
        "            self.sentences.append(sentence_clean)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # \"\"\"\n",
        "        #   Train unigram and bigram\n",
        "        # \"\"\"\n",
        "        for sentence in self.sentences:\n",
        "            sentence.insert(0, '<s>')\n",
        "            sentence.append('</s>')\n",
        "            for i in xrange(len(sentence) - 1):\n",
        "                token1 = sentence[i]\n",
        "                token2 = sentence[i + 1]\n",
        "                self.laplaceUnigramCounts[token1] += 1\n",
        "                self.laplaceBigramCounts[(token1, token2)] += 1\n",
        "                self.total += 1\n",
        "            self.total += 1\n",
        "            self.laplaceUnigramCounts[sentence[-1]] += 1\n",
        "\n",
        "\n",
        "    def candidate_word(self, word):\n",
        "        # \"\"\"\n",
        "        # Generate similar word for a given word\n",
        "        # \"\"\"\n",
        "        suggests = []\n",
        "        for candidate in self.importantKeywords:\n",
        "            if candidate.startswith(word):\n",
        "                suggests.append(candidate)\n",
        "        suggests.append(word)\n",
        "\n",
        "        if len(suggests) == 1:\n",
        "            suggests = self.d.suggest(word)\n",
        "            suggests = [suggest.lower() for suggest in suggests][:4]\n",
        "            suggests.append(word)\n",
        "            suggests = list(set(suggests))\n",
        "\n",
        "        return suggests, len(suggests)\n",
        "\n",
        "    def candidate_sentence(self, sentence):\n",
        "        # \"\"\"\n",
        "        # Takes one sentence, and return all the possible sentences, and also return a dictionary of word : suggested number of words\n",
        "        # \"\"\"\n",
        "        candidate_sentences = []\n",
        "        words_count = {}\n",
        "        for word in sentence:\n",
        "            candidate_sentences.append(self.candidate_word(word)[0])\n",
        "            words_count[word] = self.candidate_word(word)[1]\n",
        "\n",
        "        candidate_sentences = list(itertools.product(*candidate_sentences))\n",
        "        return candidate_sentences, words_count\n",
        "\n",
        "    def correction_score(self, words_count, old_sentence, new_sentence) :\n",
        "        # \"\"\"\n",
        "        #   Take a old sentence and a new sentence, for each words in the new sentence, if it's same as the orginal sentence, assign 0.95 prob\n",
        "        #   If it's not same as original sentence, give 0.05 / (count(similarword) - 1)\n",
        "        # \"\"\"\n",
        "        score = 1\n",
        "        for i in xrange(len(new_sentence)) :\n",
        "            if new_sentence[i] in words_count :\n",
        "                score *= 0.95\n",
        "            else :\n",
        "                score *= (0.05 / (words_count[old_sentence[i]] - 1))\n",
        "        return math.log(score)\n",
        "\n",
        "    def score(self, sentence):\n",
        "        # \"\"\"\n",
        "        #     Takes a list of strings as argument and returns the log-probability of the\n",
        "        #     sentence using the stupid backoff language model.\n",
        "        #     Use laplace smoothing to avoid new words with 0 probability\n",
        "        # \"\"\"\n",
        "        score = 0.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            if self.laplaceBigramCounts[(sentence[i],sentence[i + 1])] > 0:\n",
        "                score += math.log(self.laplaceBigramCounts[(sentence[i],sentence[i + 1])])\n",
        "                score -= math.log(self.laplaceUnigramCounts[sentence[i]])\n",
        "            else:\n",
        "                score += (math.log(self.laplaceUnigramCounts[sentence[i + 1]] + 1) + math.log(0.4))\n",
        "                score -= math.log(self.total + len(self.laplaceUnigramCounts))\n",
        "        return score\n",
        "\n",
        "    def return_best_sentence(self, old_sentence) :\n",
        "        # \"\"\"\n",
        "        #   Generate all candiate sentences and\n",
        "        #   Calculate the prob of each one and return the one with highest probability\n",
        "        #   Probability involves two part 1. correct probability and 2. language model prob\n",
        "        #   correct prob : p(c | w)\n",
        "        #   language model prob : use stupid backoff algorithm\n",
        "        # \"\"\"\n",
        "        bestScore = float('-inf')\n",
        "        bestSentence = []\n",
        "        old_sentence = [word.lower() for word in old_sentence.split()]\n",
        "        sentences, word_count = self.candidate_sentence(old_sentence)\n",
        "        for new_sentence in sentences:\n",
        "            new_sentence = list(new_sentence)\n",
        "            score = self.correction_score(word_count, new_sentence, old_sentence)\n",
        "            new_sentence.insert(0, '<s>')\n",
        "            new_sentence.append('</s>')\n",
        "            score += self.score(new_sentence)\n",
        "            if score >= bestScore:\n",
        "                bestScore = score\n",
        "                bestSentence = new_sentence\n",
        "        bestSentence = ' '.join(bestSentence[1:-1])\n",
        "        return bestSentence, bestScore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_NBwBgW-R5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "a26ac0d1-cc6d-486e-9752-9cf0eea2cad9"
      },
      "source": [
        "\n",
        "\n",
        "# ====== Usage Examples ========\n",
        "corrector = Sentence_Corrector('cats.txt')\n",
        "print(\"======= Non-word Correction ===========\")\n",
        "print(corrector.return_best_sentence('this is wron spallin word'))\n",
        "print(corrector.return_best_sentence('aoccdrning to a resarch at cmabridge university'))\n",
        "print(corrector.return_best_sentence('it does not mttaer in waht oredr the ltteers'))\n",
        "print(corrector.return_best_sentence('the olny important tihng is taht'))\n",
        "print(corrector.return_best_sentence('Can they leav him my messages'))\n",
        "\n",
        "print(\"======= Word Correction =========\")\n",
        "print(corrector.return_best_sentence('My marriage had drafted use away from each otter.'))\n",
        "print(corrector.return_best_sentence('it dose no matte in what order he letters re'))\n",
        "print(corrector.return_best_sentence('he only important ting us hat'))\n",
        "print(corrector.return_best_sentence('Can the eave hi my messages'))\n",
        "print(corrector.return_best_sentence('Apples grown from sed ten toe bee different from arpents'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-60f23589a10b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ====== Usage Examples ========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcorrector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence_Corrector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cats.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"======= Non-word Correction ===========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_best_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this is wron spallin word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-12ceaec09d4d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportantKeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_US\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-12ceaec09d4d>\u001b[0m in \u001b[0;36mtokenize_file\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#   Read the file, tokenize and build a list of sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtMhId4I-R85"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}